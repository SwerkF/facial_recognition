#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de rÃ©entraÃ®nement du modÃ¨le avec corrections
Corrige le problÃ¨me des prÃ©dictions nulles

Author: GitHub Copilot
Date: 2025-07-31
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt

class FixedModelTrainer:
    def __init__(self):
        self.seed = 1
        self.batch_size = 32
        self.image_size = (128, 128)
        self.epochs = 10  # Plus d'Ã©poques
        self.selected_user = "damien"
        
        # Configuration fixe pour Ã©viter les problÃ¨mes
        tf.random.set_seed(self.seed)
        np.random.seed(self.seed)
    
    def create_corrected_dataset(self):
        """CrÃ©e un dataset avec normalisation correcte"""
        print("ğŸ“‚ Chargement du dataset avec normalisation fixe...")
        
        try:
            train_ds = tf.keras.preprocessing.image_dataset_from_directory(
                './binary_dataset',
                validation_split=0.2,
                subset='training',
                seed=self.seed,
                label_mode='binary',
                batch_size=self.batch_size,
                image_size=self.image_size
            )
            
            val_ds = tf.keras.preprocessing.image_dataset_from_directory(
                './binary_dataset',
                validation_split=0.2,
                subset='validation',
                seed=self.seed,
                label_mode='binary',
                batch_size=self.batch_size,
                image_size=self.image_size
            )
            
            # NORMALISATION CORRECTE ET EXPLICITE
            def normalize_and_verify(image, label):
                # Convertir en float32 et normaliser
                image = tf.cast(image, tf.float32) / 255.0
                
                # VÃ©rifier les valeurs (debug)
                tf.debugging.assert_all_finite(image, "Image contient des valeurs non-finies")
                tf.debugging.assert_greater_equal(image, 0.0, "Image contient des valeurs nÃ©gatives")
                tf.debugging.assert_less_equal(image, 1.0, "Image contient des valeurs > 1")
                
                return image, label
            
            train_ds = train_ds.map(normalize_and_verify, num_parallel_calls=tf.data.AUTOTUNE)
            val_ds = val_ds.map(normalize_and_verify, num_parallel_calls=tf.data.AUTOTUNE)
            
            # Optimisation des performances
            train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)
            val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)
            
            print(f"âœ… Dataset crÃ©Ã©:")
            print(f"   Batch size: {self.batch_size}")
            
            return train_ds, val_ds
            
        except Exception as e:
            print(f"âŒ Erreur crÃ©ation dataset: {e}")
            return None, None
    
    def create_improved_model(self):
        """CrÃ©e un modÃ¨le avec une architecture amÃ©liorÃ©e"""
        print("ğŸ§  CrÃ©ation du modÃ¨le amÃ©liorÃ©...")
        
        model = Sequential([
            # PremiÃ¨re couche Conv
            Conv2D(32, (3,3), padding='same', activation='relu', 
                   input_shape=(*self.image_size, 3),
                   kernel_initializer='he_normal'),
            BatchNormalization(),
            MaxPooling2D((2,2)),
            
            # DeuxiÃ¨me couche Conv
            Conv2D(64, (3,3), padding='same', activation='relu',
                   kernel_initializer='he_normal'),
            BatchNormalization(),
            MaxPooling2D((2,2)),
            
            # TroisiÃ¨me couche Conv
            Conv2D(128, (3,3), padding='same', activation='relu',
                   kernel_initializer='he_normal'),
            BatchNormalization(),
            MaxPooling2D((2,2)),
            
            # Couches denses avec moins de dropout
            Flatten(),
            Dense(256, activation='relu', kernel_initializer='he_normal'),
            Dropout(0.3),  # Moins de dropout
            Dense(128, activation='relu', kernel_initializer='he_normal'),
            Dropout(0.2),
            
            # Couche de sortie CRITIQUE
            Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')
        ])
        
        # COMPILATION AVEC PARAMÃˆTRES OPTIMISÃ‰S
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # LR plus Ã©levÃ©
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        print("âœ… ModÃ¨le crÃ©Ã© et compilÃ©")
        model.summary()
        
        return model
    
    def train_fixed_model(self, train_ds, val_ds):
        """EntraÃ®ne le modÃ¨le avec des paramÃ¨tres fixes"""
        print("ğŸš€ DÃ©but de l'entraÃ®nement corrigÃ©...")
        
        # CrÃ©er le modÃ¨le
        model = self.create_improved_model()
        
        # Callbacks amÃ©liorÃ©s
        checkpoint_path = f"./models/fixed_model_{self.selected_user}.keras"
        os.makedirs("./models", exist_ok=True)
        
        callbacks = [
            ModelCheckpoint(
                filepath=checkpoint_path,
                monitor='val_accuracy',
                save_best_only=True,
                save_weights_only=False,
                mode='max',
                verbose=1
            ),
            EarlyStopping(
                monitor='val_loss',
                patience=5,  # Plus de patience
                restore_best_weights=True,
                verbose=1
            )
        ]
        
        # ENTRAÃNEMENT
        history = model.fit(
            train_ds,
            epochs=self.epochs,
            validation_data=val_ds,
            callbacks=callbacks,
            verbose=1
        )
        
        print(f"âœ… EntraÃ®nement terminÃ©!")
        print(f"ğŸ“ ModÃ¨le sauvegardÃ©: {checkpoint_path}")
        
        return model, history
    
    def test_new_model(self, model):
        """Teste le nouveau modÃ¨le"""
        print("\nğŸ§ª TEST DU NOUVEAU MODÃˆLE:")
        print("-" * 40)
        
        # Tester sur images USER
        user_dir = './binary_dataset/user'
        if os.path.exists(user_dir):
            user_images = [f for f in os.listdir(user_dir) 
                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:3]
            
            print("ğŸ“Š Test images USER:")
            for img_file in user_images:
                img_path = os.path.join(user_dir, img_file)
                
                # Preprocessing identique Ã  l'entraÃ®nement
                img = tf.keras.preprocessing.image.load_img(img_path, target_size=self.image_size)
                img_array = tf.keras.preprocessing.image.img_to_array(img)
                img_array = img_array / 255.0  # Normalisation identique
                img_array = np.expand_dims(img_array, axis=0)
                
                # PrÃ©diction
                pred = model.predict(img_array, verbose=0)[0][0]
                status = "âœ… DAMIEN" if pred >= 0.5 else "âŒ AUTRE"
                print(f"   {img_file}: {pred:.4f} {status}")
        
        # Tester sur images OTHERS
        others_dir = './binary_dataset/others'
        if os.path.exists(others_dir):
            others_images = [f for f in os.listdir(others_dir) 
                            if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:3]
            
            print("ğŸ“Š Test images OTHERS:")
            for img_file in others_images:
                img_path = os.path.join(others_dir, img_file)
                
                # Preprocessing
                img = tf.keras.preprocessing.image.load_img(img_path, target_size=self.image_size)
                img_array = tf.keras.preprocessing.image.img_to_array(img)
                img_array = img_array / 255.0
                img_array = np.expand_dims(img_array, axis=0)
                
                # PrÃ©diction
                pred = model.predict(img_array, verbose=0)[0][0]
                status = "âœ… AUTRE" if pred < 0.5 else "âŒ DAMIEN"
                print(f"   {img_file}: {pred:.4f} {status}")

def main():
    """Fonction principale de rÃ©entraÃ®nement"""
    print("ğŸ”§ RÃ‰ENTRAÃNEMENT DU MODÃˆLE FACIAL")
    print("=" * 50)
    
    trainer = FixedModelTrainer()
    
    # 1. CrÃ©er le dataset corrigÃ©
    train_ds, val_ds = trainer.create_corrected_dataset()
    if train_ds is None:
        print("âŒ Impossible de crÃ©er le dataset")
        return
    
    # 2. EntraÃ®ner le modÃ¨le
    model, history = trainer.train_fixed_model(train_ds, val_ds)
    
    # 3. Tester le nouveau modÃ¨le
    trainer.test_new_model(model)
    
    # 4. Afficher les rÃ©sultats
    if history:
        final_acc = history.history['val_accuracy'][-1]
        final_loss = history.history['val_loss'][-1]
        print(f"\nğŸ“ˆ RÃ©sultats finaux:")
        print(f"   PrÃ©cision validation: {final_acc:.2%}")
        print(f"   Perte validation: {final_loss:.4f}")
    
    print("\nâœ… RÃ©entraÃ®nement terminÃ©!")
    print("ğŸ’¡ Utilisez maintenant: fixed_model_damien.h5")

if __name__ == "__main__":
    main()
