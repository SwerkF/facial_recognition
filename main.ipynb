{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0b8c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14856753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    " \n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True),\n",
    "    ModelCheckpoint(filepath='model.keras', save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb7b804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données d'entraienement \n",
    "\n",
    "def load_data():\n",
    "    # Initialiser les listes pour les données et les labels\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Dossier contenant les images d'entraînement et de test\n",
    "    #for photos in ['oliwer', 'damien']:\n",
    "    for photos in ['oliwer']:\n",
    "        train = f'processed/{photos}/train/' # Images d'entraînement\n",
    "\n",
    "        for file in os.listdir(train):\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                img = os.path.join(train, file) # Lire le chemin de l'image\n",
    "                image = cv2.imread(img, cv2.IMREAD_GRAYSCALE) # Transformer l'image en gris\n",
    "                train_data.append(image) # Ajouter l'image à la liste des données d'entraînement\n",
    "                train_labels.append(photos) # Ajouter le label correspondant\n",
    "\n",
    "        for file in os.listdir(f'processed/{photos}/test/'):\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                img = os.path.join(f'processed/{photos}/test/', file) # Lire le chemin de l'image de test\n",
    "                image = cv2.imread(img, cv2.IMREAD_GRAYSCALE) # Transformer l'image en gris\n",
    "                test_data.append(image) # Ajouter l'image à la liste des données de test\n",
    "                test_labels.append(photos) # Ajouter le label correspondant\n",
    "\n",
    "    # Données aléatoires\n",
    "    random_dir = './lfw-deepfunneled/lfw-deepfunneled'  # Dossier avec des images aléatoires\n",
    "\n",
    "    # Parcourir les sous-dossiers pour ajouter des images aléatoires\n",
    "    random_images = []\n",
    "    for person in os.listdir(random_dir):\n",
    "        person_path = os.path.join(random_dir, person) # Chemin du sous-dossier de la personne\n",
    "        if os.path.isdir(person_path):\n",
    "            for filename in os.listdir(person_path):\n",
    "                img_path = os.path.join(person_path, filename) # Chemin complet de l'image\n",
    "                if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) # Transformer l'image en gris\n",
    "                    image = cv2.resize(image, (128, 128)) # Redimensionner l'image à 100x100 pixels\n",
    "                    random_images.append(image) # Ajouter l'image à la liste des images aléatoires\n",
    "\n",
    "    # Sélectionner 80 images pour l'entraînement et 20 pour le test\n",
    "    np.random.shuffle(random_images)\n",
    "    train_data.extend(random_images[:80])\n",
    "    train_labels.extend(['random'] * 80)\n",
    "    test_data.extend(random_images[80:100])\n",
    "    test_labels.extend(['random'] * 20)\n",
    "\n",
    "    return np.array(train_data), np.array(train_labels), np.array(test_data), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b21b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oliwer\\Documents\\facial_recognition\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.6281 - loss: 14.6969 - val_accuracy: 0.7500 - val_loss: 1.7848 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.8906 - loss: 0.9393 - val_accuracy: 0.9750 - val_loss: 0.0620 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.9656 - loss: 0.0812 - val_accuracy: 0.9875 - val_loss: 0.0210 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.9844 - loss: 0.0226 - val_accuracy: 1.0000 - val_loss: 5.4033e-04 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 2.6678e-04 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 1.0466e-05 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - accuracy: 1.0000 - loss: 8.5034e-04 - val_accuracy: 1.0000 - val_loss: 5.7491e-06 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 0.9969 - loss: 2.4356e-04 - val_accuracy: 1.0000 - val_loss: 8.1508e-07 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 1.6839e-04 - val_accuracy: 1.0000 - val_loss: 5.4686e-07 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.9969 - loss: 3.9584e-05 - val_accuracy: 1.0000 - val_loss: 6.6457e-07 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 0.9969 - loss: 1.5022e-04 - val_accuracy: 1.0000 - val_loss: 2.0175e-06 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 6.8874e-06 - val_accuracy: 1.0000 - val_loss: 2.2603e-06 - learning_rate: 2.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.9969 - loss: 1.6155e-05 - val_accuracy: 1.0000 - val_loss: 1.9981e-06 - learning_rate: 2.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 3.6157e-05 - val_accuracy: 1.0000 - val_loss: 1.7970e-06 - learning_rate: 2.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.9969 - loss: 1.1730e-05 - val_accuracy: 1.0000 - val_loss: 1.6927e-06 - learning_rate: 2.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.9969 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 2.5553e-06 - learning_rate: 2.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 4.6640e-07 - learning_rate: 4.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 4.0836e-04 - val_accuracy: 1.0000 - val_loss: 2.5180e-06 - learning_rate: 4.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 4.6783e-05 - val_accuracy: 1.0000 - val_loss: 5.4765e-06 - learning_rate: 4.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 5.4948e-05 - val_accuracy: 1.0000 - val_loss: 7.0046e-06 - learning_rate: 4.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 3.7783e-06 - val_accuracy: 1.0000 - val_loss: 7.4707e-06 - learning_rate: 4.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 1.5592e-04 - val_accuracy: 1.0000 - val_loss: 7.4186e-06 - learning_rate: 8.0000e-06\n",
      "Epoch 23/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.9969 - loss: 9.6307e-05 - val_accuracy: 1.0000 - val_loss: 7.2101e-06 - learning_rate: 8.0000e-06\n",
      "Epoch 24/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.9969 - loss: 3.1323e-04 - val_accuracy: 1.0000 - val_loss: 7.1475e-06 - learning_rate: 8.0000e-06\n",
      "Epoch 25/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 1.4755e-05 - val_accuracy: 1.0000 - val_loss: 6.9822e-06 - learning_rate: 8.0000e-06\n",
      "Epoch 26/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 1.2526e-05 - val_accuracy: 1.0000 - val_loss: 6.9182e-06 - learning_rate: 8.0000e-06\n",
      "Epoch 27/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 6.1750e-04 - val_accuracy: 1.0000 - val_loss: 6.8467e-06 - learning_rate: 1.6000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1eaf4cdf140>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, train_labels, test_data, test_labels = load_data() # Charger les données\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)  # Encoder les labels d'entraînement\n",
    "test_labels = label_encoder.transform(test_labels)        # Encoder les labels de test\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=3)  # Convertir en one-hot encoding\n",
    "test_labels = to_categorical(test_labels, num_classes=3)    # Convertir en one-hot encoding\n",
    "\n",
    "train_data = train_data.reshape(-1, 128, 128, 1)  # Reshape pour le modèle CNN\n",
    "test_data = test_data.reshape(-1, 128, 128, 1)    # Reshape pour le modèle CNN\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128,128,1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='sigmoid')  # 2 classes: 'oliwer', 'random'\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, epochs=50, batch_size=32, validation_data=(test_data, test_labels), callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6c106b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 351 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x000001EB10303880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 4.6640e-07 \n",
      "Test Loss: 4.664024402245559e-07, Test Accuracy: 1.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Predicted label: oliwer\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model('modelSigmoid.keras')  # Charger le modèle sauvegardé\n",
    "\n",
    "# Évaluation du modèle\n",
    "test_loss, test_accuracy = loaded_model.evaluate(test_data, test_labels)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "test_file = '20250731_114033_087.jpg'\n",
    "\n",
    "test_image = cv2.imread(test_file, cv2.IMREAD_GRAYSCALE)\n",
    "test_image = cv2.resize(test_image, (128, 128))\n",
    "test_image = test_image.reshape(1, 128, 128, 1)  # Reshape pour le modèle CNN\n",
    "predictions = loaded_model.predict(test_image)\n",
    "predicted_label = label_encoder.inverse_transform([np.argmax(predictions[0])])\n",
    "print(f\"Predicted label: {predicted_label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
