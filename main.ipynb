{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "# TensorFlow et Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a547dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du système\n",
    "seed = 1\n",
    "batch_size = 32\n",
    "image_size = (128, 128)\n",
    "epochs = 10\n",
    "selected_user = \"damien\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3139dd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset configuré:\n",
      "   Photos de damien: 300\n",
      "   Photos d'autres: 900\n",
      "Système configuré pour: damien\n"
     ]
    }
   ],
   "source": [
    "# Configuration utilisateur et préparation du dataset\n",
    "import shutil\n",
    "\n",
    "def setup_binary_dataset():\n",
    "    \"\"\"Configure le dataset binaire pour la reconnaissance de Damien\"\"\"\n",
    "    # Dossiers source et destination\n",
    "    processed_dir = './processed/' + selected_user\n",
    "    binary_dataset_dir = './binary_dataset'\n",
    "    user_dir = os.path.join(binary_dataset_dir, 'user')\n",
    "    others_dir = os.path.join(binary_dataset_dir, 'others')\n",
    "    \n",
    "    # Créer les dossiers si nécessaire\n",
    "    os.makedirs(user_dir, exist_ok=True)\n",
    "    os.makedirs(others_dir, exist_ok=True)\n",
    "    \n",
    "    # Nettoyer le dossier user actuel\n",
    "    for file in os.listdir(user_dir):\n",
    "        file_path = os.path.join(user_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "    \n",
    "    # Copier les photos\n",
    "    copied_count = 0\n",
    "    for folder in ['train', 'test']:\n",
    "        source_folder = os.path.join(processed_dir, folder)\n",
    "        if os.path.exists(source_folder):\n",
    "            photos = [f for f in os.listdir(source_folder) \n",
    "                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            for photo in photos:\n",
    "                src = os.path.join(source_folder, photo)\n",
    "                dst = os.path.join(user_dir, f\"{folder}_{photo}\")\n",
    "                try:\n",
    "                    shutil.copy2(src, dst)\n",
    "                    copied_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur {photo}: {e}\")\n",
    "    \n",
    "    # Statistiques\n",
    "    user_count = len([f for f in os.listdir(user_dir) \n",
    "                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    others_count = len([f for f in os.listdir(others_dir) \n",
    "                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    print(f\"Dataset configuré:\")\n",
    "    print(f\"   Photos de {selected_user}: {user_count}\")\n",
    "    print(f\"   Photos d'autres: {others_count}\")\n",
    "    \n",
    "    return user_count > 0 and others_count > 0\n",
    "\n",
    "dataset_ready = setup_binary_dataset()\n",
    "\n",
    "if dataset_ready:\n",
    "    print(f\"Système configuré pour: {selected_user}\")\n",
    "else:\n",
    "    print(\"Problème de configuration du dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959709f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset avec normalisation correcte...\n",
      "Found 1200 files belonging to 2 classes.\n",
      "Using 960 files for training.\n",
      "Found 1200 files belonging to 2 classes.\n",
      "Using 240 files for validation.\n",
      "Dataset chargé avec succès\n",
      "   Classes: ['others', 'user']\n",
      "   Distribution: USER=300, OTHERS=900\n"
     ]
    }
   ],
   "source": [
    "# Chargement du dataset pour classification binaire\n",
    "def load_binary_dataset():\n",
    "    \"\"\"Charge le dataset pour classification binaire avec normalisation correcte\"\"\"\n",
    "    binary_data_dir = './binary_dataset'\n",
    "    \n",
    "    if not os.path.exists(binary_data_dir):\n",
    "        print(\"Dossier binary_dataset non trouvé!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Vérifier la présence d'images\n",
    "    user_dir = os.path.join(binary_data_dir, 'user')\n",
    "    others_dir = os.path.join(binary_data_dir, 'others')\n",
    "    \n",
    "    user_photos = len([f for f in os.listdir(user_dir) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) if os.path.exists(user_dir) else 0\n",
    "    others_photos = len([f for f in os.listdir(others_dir) \n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) if os.path.exists(others_dir) else 0\n",
    "    \n",
    "    if user_photos == 0 or others_photos == 0:\n",
    "        print(f\"Dataset incomplet: USER={user_photos}, OTHERS={others_photos}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(\"Chargement du dataset avec normalisation correcte...\")\n",
    "        \n",
    "        train_ds = image_dataset_from_directory(\n",
    "            binary_data_dir,\n",
    "            validation_split=0.2,\n",
    "            subset='training',\n",
    "            seed=seed,\n",
    "            label_mode='binary',\n",
    "            batch_size=batch_size,\n",
    "            image_size=image_size\n",
    "        )\n",
    "    \n",
    "        validation_ds = image_dataset_from_directory(\n",
    "            binary_data_dir,\n",
    "            validation_split=0.2,\n",
    "            subset='validation',\n",
    "            seed=seed,\n",
    "            label_mode='binary',\n",
    "            batch_size=batch_size,\n",
    "            image_size=image_size\n",
    "        )\n",
    "        \n",
    "        # RÉCUPÉRER LES NOMS DES CLASSES AVANT LES TRANSFORMATIONS\n",
    "        class_names = train_ds.class_names\n",
    "        \n",
    "        # NORMALISATION CORRECTE ET EXPLICITE\n",
    "        def normalize_and_verify(image, label):\n",
    "            # Convertir en float32 et normaliser\n",
    "            image = tf.cast(image, tf.float32) / 255.0\n",
    "            \n",
    "            # Vérifications de sécurité (optionnelles en production)\n",
    "            tf.debugging.assert_all_finite(image, \"Image contient des valeurs non-finies\")\n",
    "            tf.debugging.assert_greater_equal(image, 0.0, \"Image contient des valeurs négatives\")\n",
    "            tf.debugging.assert_less_equal(image, 1.0, \"Image contient des valeurs > 1\")\n",
    "            \n",
    "            return image, label\n",
    "        \n",
    "        # Appliquer la normalisation\n",
    "        train_ds = train_ds.map(normalize_and_verify, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        validation_ds = validation_ds.map(normalize_and_verify, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Optimisation des performances\n",
    "        train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "        validation_ds = validation_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(f\"Dataset chargé avec succès\")\n",
    "        print(f\"   Classes: {class_names}\")\n",
    "        print(f\"   Distribution: USER={user_photos}, OTHERS={others_photos}\")\n",
    "    \n",
    "        return train_ds, validation_ds\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Chargement automatique\n",
    "train_ds, validation_ds = load_binary_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3322c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_model():\n",
    "    \"\"\"Crée un modèle avec une architecture améliorée\"\"\"\n",
    "    print(\"Création du modèle amélioré...\")\n",
    "    \n",
    "    # Créer le modèle avec architecture optimisée\n",
    "    improved_model = Sequential([\n",
    "\n",
    "        # Première couche Conv avec initialisation he_normal\n",
    "        Conv2D(32, (3,3), padding='same', activation='relu', \n",
    "               input_shape=(*image_size, 3),\n",
    "               kernel_initializer='he_normal'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        # Deuxième couche Conv\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu',\n",
    "               kernel_initializer='he_normal'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        # Troisième couche Conv\n",
    "        Conv2D(128, (3,3), padding='same', activation='relu',\n",
    "               kernel_initializer='he_normal'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        # Couches denses avec dropout réduit\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(256, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dropout(0.3),  # Moins de dropout pour éviter l'underfitting\n",
    "        \n",
    "        Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')\n",
    "    ])\n",
    "\n",
    "    # Compilation avec optimiseur Adam optimisé\n",
    "    improved_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    print(\"Modèle amélioré créé et compilé\")\n",
    "    improved_model.summary()\n",
    "    \n",
    "    return improved_model\n",
    "\n",
    "def test_improved_model(model_to_test):\n",
    "    \"\"\"Teste le modèle amélioré sur quelques images\"\"\"\n",
    "    print(\"\\nTEST DU MODÈLE AMÉLIORÉ:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Tester sur images USER\n",
    "    user_dir = './binary_dataset/user'\n",
    "    if os.path.exists(user_dir):\n",
    "        user_images = [f for f in os.listdir(user_dir) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:3]\n",
    "        \n",
    "        print(\"Test images USER:\")\n",
    "        for img_file in user_images:\n",
    "            img_path = os.path.join(user_dir, img_file)\n",
    "            \n",
    "            # Preprocessing identique à l'entraînement\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            img_array = img_array / 255.0  # Normalisation identique\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            \n",
    "            # Prédiction\n",
    "            pred = model_to_test.predict(img_array, verbose=0)[0][0]\n",
    "            status = \"DAMIEN\" if pred >= 0.5 else \"AUTRE\"\n",
    "            print(f\"   {img_file}: {pred:.4f} {status}\")\n",
    "    \n",
    "        # Tester sur images OTHERS\n",
    "        others_dir = './binary_dataset/others'\n",
    "        if os.path.exists(others_dir):\n",
    "            others_images = [f for f in os.listdir(others_dir) \n",
    "                            if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:3]\n",
    "            \n",
    "            print(\"Test images OTHERS:\")\n",
    "            for img_file in others_images:\n",
    "                img_path = os.path.join(others_dir, img_file)\n",
    "                \n",
    "                # Preprocessing\n",
    "                img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)\n",
    "                img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                img_array = img_array / 255.0\n",
    "                img_array = np.expand_dims(img_array, axis=0)\n",
    "                \n",
    "                # Prédiction\n",
    "                pred = model_to_test.predict(img_array, verbose=0)[0][0]\n",
    "                status = \"AUTRE\" if pred < 0.5 else \"DAMIEN\"\n",
    "                print(f\"   {img_file}: {pred:.4f} {status}\")\n",
    "\n",
    "    print(\"Fonctions de modèle créées\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2bac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que le dataset est chargé\n",
    "if train_ds is None or validation_ds is None:\n",
    "    print(\"Dataset non disponible. Rechargement...\")\n",
    "    train_ds, validation_ds = load_binary_dataset()\n",
    "\n",
    "if train_ds is None:\n",
    "    print(\"Impossible de charger le dataset\")\n",
    "else:\n",
    "    print(\"Démarrage de l'entraînement...\")\n",
    "    \n",
    "    # Créer le modèle amélioré\n",
    "    model_fixed = create_improved_model()\n",
    "    \n",
    "    # Configuration des callbacks\n",
    "    checkpoint_path_fixed = f\"./models/fixed_model_{selected_user}.keras\"\n",
    "    os.makedirs(\"./models\", exist_ok=True)\n",
    "    \n",
    "    callbacks_fixed = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=checkpoint_path_fixed,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    print(f\"Modèle sera sauvegardé: {checkpoint_path_fixed}\")\n",
    "    \n",
    "    # Entraînement\n",
    "    history_fixed = model_fixed.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_ds,\n",
    "        callbacks=callbacks_fixed,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Test du nouveau modèle\n",
    "    print(\"\\nTest du modèle:\")\n",
    "    test_improved_model(model_fixed)\n",
    "    \n",
    "    # Affichage des résultats finaux\n",
    "    if history_fixed:\n",
    "        final_acc_fixed = history_fixed.history['val_accuracy'][-1]\n",
    "        final_loss_fixed = history_fixed.history['val_loss'][-1]\n",
    "        \n",
    "        print(f\"\\nRÉSULTATS FINAUX:\")\n",
    "        print(f\"Précision validation: {final_acc_fixed:.2%}\")\n",
    "        print(f\"Perte validation: {final_loss_fixed:.4f}\")\n",
    "        \n",
    "        # Comparer avec l'ancien modèle si disponible\n",
    "        if 'final_val_acc' in globals():\n",
    "            improvement = final_acc_fixed - final_val_acc\n",
    "            print(f\"Amélioration: {improvement:+.2%} ancien modèle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d898b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des résultats d'entraînement\n",
    "if 'history_fixed' in globals() and history_fixed is not None:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Graphique de précision\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_fixed.history['accuracy'], label='Précision Entraînement', marker='o')\n",
    "    plt.plot(history_fixed.history['val_accuracy'], label='Précision Validation', marker='s')\n",
    "    plt.title('Évolution de la Précision')\n",
    "    plt.xlabel('Époques')\n",
    "    plt.ylabel('Précision')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Graphique de perte\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_fixed.history['loss'], label='Perte Entraînement', marker='o')\n",
    "    plt.plot(history_fixed.history['val_loss'], label='Perte Validation', marker='s')\n",
    "    plt.title('Évolution de la Perte')\n",
    "    plt.xlabel('Époques')\n",
    "    plt.ylabel('Perte')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Affichage des métriques finales\n",
    "    final_train_acc = history_fixed.history['accuracy'][-1]\n",
    "    final_val_acc = history_fixed.history['val_accuracy'][-1]\n",
    "    final_train_loss = history_fixed.history['loss'][-1]\n",
    "    final_val_loss = history_fixed.history['val_loss'][-1]\n",
    "    \n",
    "else:\n",
    "    print(\"Aucun historique d'entraînement disponible\")\n",
    "    print(\"Exécutez d'abord la cellule d'entraînement corrigé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43e95954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonctions de test final créées\n"
     ]
    }
   ],
   "source": [
    "# Fonctions de test final\n",
    "def load_best_model():\n",
    "    \"\"\"Charge le meilleur modèle disponible\"\"\"\n",
    "    # Essayer d'abord le modèle corrigé\n",
    "    fixed_model_path = f\"./models/fixed_model_{selected_user}.keras\"\n",
    "    old_model_path = f\"./models/best_model_{selected_user}.h5\"\n",
    "    \n",
    "    global final_model\n",
    "    \n",
    "    if os.path.exists(fixed_model_path):\n",
    "        try:\n",
    "            final_model = tf.keras.models.load_model(fixed_model_path)\n",
    "            print(f\"Modèle corrigé chargé: {fixed_model_path}\")\n",
    "            print(f\"Configuré pour: {selected_user}\")\n",
    "            return \"fixed\"\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur chargement modèle corrigé: {e}\")\n",
    "    \n",
    "    if os.path.exists(old_model_path):\n",
    "        try:\n",
    "            final_model = tf.keras.models.load_model(old_model_path)\n",
    "            print(f\"Ancien modèle chargé: {old_model_path}\")\n",
    "            print(f\"Configuré pour: {selected_user}\")\n",
    "            print(f\"Recommandation: Utilisez le modèle corrigé pour de meilleurs résultats\")\n",
    "            return \"old\"\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur chargement ancien modèle: {e}\")\n",
    "    \n",
    "    print(f\"Aucun modèle trouvé!\")\n",
    "    print(f\"Exécutez d'abord l'entraînement ou le réentraînement corrigé\")\n",
    "    final_model = None\n",
    "    return None\n",
    "\n",
    "def preprocess_final_image(image_path):\n",
    "    \"\"\"Préprocesse une image pour la prédiction (identique à l'entraînement)\"\"\"\n",
    "    try:\n",
    "        # Charger l'image\n",
    "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=image_size)\n",
    "        \n",
    "        # Convertir en array\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        \n",
    "        # Normalisation IDENTIQUE à l'entraînement\n",
    "        img_array = img_array / 255.0\n",
    "        \n",
    "        # Ajouter dimension batch\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur preprocessing: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_final(image_path, threshold=0.5):\n",
    "    \"\"\"Prédiction avec le modèle final\"\"\"\n",
    "    if 'final_model' not in globals() or final_model is None:\n",
    "        print(\"Aucun modèle chargé\")\n",
    "        return None\n",
    "    \n",
    "    # Preprocessing\n",
    "    img_array = preprocess_final_image(image_path)\n",
    "    if img_array is None:\n",
    "        return None\n",
    "    \n",
    "    # Prédiction\n",
    "    prediction = final_model.predict(img_array, verbose=0)[0][0]\n",
    "    \n",
    "    # Décision\n",
    "    is_user = prediction >= threshold\n",
    "    \n",
    "    return {\n",
    "        'image_path': image_path,\n",
    "        'probability': float(prediction),\n",
    "        'is_user': bool(is_user),\n",
    "        'confidence': float(max(prediction, 1 - prediction)),\n",
    "        'user_name': selected_user if is_user else 'autre',\n",
    "        'decision': 1 if is_user else 0\n",
    "    }\n",
    "\n",
    "def test_single_image_final(image_path, threshold=0.5):\n",
    "    \"\"\"Test une image avec affichage détaillé des résultats\"\"\"\n",
    "        \n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image non trouvée: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    result = predict_final(image_path, threshold)\n",
    "    if result:\n",
    "        status = \"DAMIEN DETECTE\" if result['is_user'] else \"AUTRE PERSONNE\"\n",
    "        \n",
    "        print(f\"Résultat: {status}\")\n",
    "        print(f\"Probabilité: {result['probability']:.4f}\")\n",
    "        print(f\"Confiance: {result['confidence']:.1%}\")\n",
    "        print(f\"Seuil utilisé: {threshold}\")\n",
    "        \n",
    "        # Suggestions de seuils\n",
    "        print(f\"\\nTest avec différents seuils:\")\n",
    "        for t in [0.3, 0.5, 0.7, 0.9]:\n",
    "            decision = \"DAMIEN\" if result['probability'] >= t else \"AUTRE\"\n",
    "            correct = (result['probability'] >= t) == result['is_user']\n",
    "            status_icon = \"CORRECT\" if correct else \"DIFFERENT\"\n",
    "            print(f\"   Seuil {t}: {status_icon} {decision}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def test_multiple_images_final():\n",
    "    \"\"\"Test automatique sur plusieurs images du dataset\"\"\"\n",
    "    print(\"\\nTESTS AUTOMATIQUES:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test images USER\n",
    "    user_dir = './binary_dataset/user'\n",
    "    if os.path.exists(user_dir):\n",
    "        user_images = [f for f in os.listdir(user_dir) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:3]\n",
    "        \n",
    "        print(\"Test images USER (doivent être détectées comme DAMIEN):\")\n",
    "        for img_file in user_images:\n",
    "            img_path = os.path.join(user_dir, img_file)\n",
    "            result = predict_final(img_path)\n",
    "            if result:\n",
    "                status = \"CORRECT\" if result['is_user'] else \"ERREUR\"\n",
    "                print(f\"   {img_file}: {result['probability']:.4f} → {status}\")\n",
    "    \n",
    "    # Test images OTHERS\n",
    "    others_dir = './binary_dataset/others'\n",
    "    if os.path.exists(others_dir):\n",
    "        others_images = [f for f in os.listdir(others_dir) \n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:3]\n",
    "        \n",
    "        print(\"\\nTest images OTHERS (doivent être détectées comme AUTRE):\")\n",
    "        for img_file in others_images:\n",
    "            img_path = os.path.join(others_dir, img_file)\n",
    "            result = predict_final(img_path)\n",
    "            if result:\n",
    "                status = \"CORRECT\" if not result['is_user'] else \"ERREUR\"\n",
    "                print(f\"   {img_file}: {result['probability']:.4f} → {status}\")\n",
    "\n",
    "print(\"Fonctions de test final créées\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e45dea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle corrigé chargé: ./models/fixed_model_damien.keras\n",
      "Configuré pour: damien\n",
      "\n",
      "Modèle chargé avec succès (fixed)\n",
      "\n",
      "TESTS AUTOMATIQUES:\n",
      "============================================================\n",
      "Test images USER (doivent être détectées comme DAMIEN):\n",
      "   test_dark_20250731_114225_003.jpg: 1.0000 → CORRECT\n",
      "   test_dark_20250731_114225_004.jpg: 1.0000 → CORRECT\n",
      "   test_dark_20250731_114225_005.jpg: 1.0000 → CORRECT\n",
      "\n",
      "Test images OTHERS (doivent être détectées comme AUTRE):\n",
      "   Alessandra_Cerna_Alessandra_Cerna_0001.jpg: 0.0000 → CORRECT\n",
      "   Alexander_Downer_Alexander_Downer_0002.jpg: 0.0000 → CORRECT\n",
      "   Alexandra_Vodjanikova_Alexandra_Vodjanikova_0001.jpg: 0.0000 → CORRECT\n"
     ]
    }
   ],
   "source": [
    "model_type = load_best_model()\n",
    "\n",
    "if model_type:\n",
    "    print(f\"\\nModèle chargé avec succès ({model_type})\")\n",
    "    # 2. Tests automatiques sur le dataset\n",
    "    test_multiple_images_final()\n",
    "else:\n",
    "    print(\"\\nÉchec du chargement du modèle\")\n",
    "    print(\"Assurez-vous d'avoir exécuté l'entraînement corrigé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c51ff09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 photo(s) trouvée(s):\n",
      "   1Média (1).jpg\n",
      "   1Média.jpg\n",
      "   200px-Trevor.png\n",
      "   Albert-Einstein.jpg\n",
      "   image.png\n",
      "   Média (2).jpg\n",
      "   Média (3).jpg\n",
      "   Média.jpg\n",
      "   téléchargement (1).jpg\n",
      "   téléchargement (2).jpg\n",
      "   téléchargement (3).jpg\n",
      "   téléchargement (4).jpg\n",
      "   téléchargement.jpg\n",
      "   téléchargement2.jpg\n",
      "   untitled (1).png\n",
      "   untitled@2x (2).png\n",
      "   untitled@2x (3).png\n",
      "\n",
      "Pour tester une photo, utilisez:\n",
      "test_single_image_final('./processed/test_photos/1Média (1).jpg')\n",
      "Résultat: AUTRE PERSONNE\n",
      "Probabilité: 0.1978\n",
      "Confiance: 80.2%\n",
      "Seuil utilisé: 0.5\n",
      "\n",
      "Test avec différents seuils:\n",
      "   Seuil 0.3: CORRECT AUTRE\n",
      "   Seuil 0.5: CORRECT AUTRE\n",
      "   Seuil 0.7: CORRECT AUTRE\n",
      "   Seuil 0.9: CORRECT AUTRE\n",
      "\n",
      "Prêt pour vos tests personnalisés!\n"
     ]
    }
   ],
   "source": [
    "test_photos_dir = './processed/test_photos'\n",
    "os.makedirs(test_photos_dir, exist_ok=True)\n",
    "\n",
    "if os.path.exists(test_photos_dir):\n",
    "    photos = [f for f in os.listdir(test_photos_dir) \n",
    "              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    if photos:\n",
    "        print(f\"{len(photos)} photo(s) trouvée(s):\")\n",
    "        for photo in photos:\n",
    "            print(f\"   {photo}\")\n",
    "        \n",
    "        print(\"\\nPour tester une photo, utilisez:\")\n",
    "        print(f\"test_single_image_final('{test_photos_dir}/{photos[0]}')\")\n",
    "    else:\n",
    "        print(\"Aucune photo dans le dossier de test\")\n",
    "        print(\"Copiez vos photos dans ce dossier puis réexécutez cette cellule\")\n",
    "else:\n",
    "    print(\"Impossible de créer le dossier de test\")\n",
    "\n",
    "test_single_image_final(\"./processed/test_photos/Média (2).jpg\")\n",
    "\n",
    "print(\"\\nPrêt pour vos tests personnalisés!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du Modèle - Version graphique uniquement\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def evaluate_model_visual_only():\n",
    "    \"\"\"Évalue le modèle avec visualisations uniquement (sans prints)\"\"\"\n",
    "    \n",
    "    # Vérifications silencieuses\n",
    "    if 'model_fixed' not in globals() or model_fixed is None:\n",
    "        return\n",
    "    if validation_ds is None:\n",
    "        return\n",
    "    \n",
    "    # Extraction des données\n",
    "    y_true = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    for batch_images, batch_labels in validation_ds:\n",
    "        batch_predictions = model_fixed.predict(batch_images, verbose=0)\n",
    "        y_true.extend(batch_labels.numpy().flatten())\n",
    "        y_pred_proba.extend(batch_predictions.flatten())\n",
    "    \n",
    "    # Conversion en arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Classes\n",
    "    class_names = ['Autre', 'Damien']\n",
    "    \n",
    "    # VISUALISATIONS\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # 1. Matrice de confusion\n",
    "    plt.subplot(2, 3, 1)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Échantillons'})\n",
    "    plt.title('Matrice de Confusion')\n",
    "    plt.xlabel('Prédiction')\n",
    "    plt.ylabel('Vérité')\n",
    "    \n",
    "    # 2. Matrice de confusion normalisée\n",
    "    plt.subplot(2, 3, 2)\n",
    "    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(conf_matrix_norm, annot=True, fmt='.2%', cmap='Oranges',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Pourcentage'})\n",
    "    plt.title('Matrice Normalisée')\n",
    "    plt.xlabel('Prédiction')\n",
    "    plt.ylabel('Vérité')\n",
    "\n",
    "# Exécution\n",
    "results = evaluate_model_visual_only()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
