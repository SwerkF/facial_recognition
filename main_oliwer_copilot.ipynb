{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a0b8c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération de données d'entraînement avec augmentation\n",
    "\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "14856753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    " \n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True),\n",
    "    ModelCheckpoint(filepath='model.keras', save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44510f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(image):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    image_normalized = image.astype('float32') / 255.0\n",
    "    image_reshaped = np.expand_dims(image_normalized, axis=-1)\n",
    "    image_batch = np.expand_dims(image_reshaped, axis=0)\n",
    "    \n",
    "    # Générer 10 images augmentées par image originale\n",
    "    augmented_count = 0\n",
    "    for batch in datagen.flow(image_batch, batch_size=1):\n",
    "        if augmented_count >= 10:  # Limiter à 10 images augmentées par image\n",
    "            break\n",
    "        \n",
    "        augmented_image = batch[0]\n",
    "        augmented_image = (augmented_image.squeeze() * 255).astype('uint8')\n",
    "        \n",
    "        train_data.append(augmented_image)\n",
    "        train_labels.append(1)\n",
    "        augmented_count += 1\n",
    "\n",
    "    return train_data, train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données d'entraienement \n",
    "\n",
    "def load_data():\n",
    "    # Initialiser les listes pour les données et les labels\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Données oliwer\n",
    "    for photos in ['oliwer']:\n",
    "        train = f'processed/{photos}/train/' # Images d'entraînement\n",
    "\n",
    "        for file in os.listdir(train):\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                img = os.path.join(train, file) # Lire le chemin de l'image\n",
    "                image = cv2.imread(img, cv2.IMREAD_GRAYSCALE) # Transformer l'image en gris\n",
    "                image = cv2.resize(image, (96, 96)) # Redimensionner l'image à 96x96 pixels\n",
    "                \n",
    "                # Ajouter l'image originale\n",
    "                train_data.append(image)\n",
    "                train_labels.append(1) # 1 = Oliwer\n",
    "                \n",
    "                # Générer des images augmentées\n",
    "                augmented_data, augmented_labels = generate_batch(image)\n",
    "                train_data.extend(augmented_data)\n",
    "                train_labels.extend(augmented_labels)\n",
    "\n",
    "        for file in os.listdir(f'processed/{photos}/test/'):\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                img = os.path.join(f'processed/{photos}/test/', file) # Lire le chemin de l'image de test\n",
    "                image = cv2.imread(img, cv2.IMREAD_GRAYSCALE) # Transformer l'image en gris\n",
    "                image = cv2.resize(image, (96, 96)) # Redimensionner l'image à 64x64 pixels\n",
    "                test_data.append(image) # Ajouter l'image à la liste des données de test\n",
    "                test_labels.append(1) # Ajouter le label correspondant\n",
    "\n",
    "    # Données aléatoires\n",
    "    random_train_dir = './processed/non_oliwer/train'  # Dossier avec des images aléatoires d'entraînement\n",
    "    random_test_dir = './processed/non_oliwer/test'    # Dossier avec des images aléatoires de test\n",
    "    \n",
    "    # Traitement des images d'entraînement aléatoires (limité à 240)\n",
    "    random_train_files = [f for f in os.listdir(random_train_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    for file in random_train_files[:2400]:\n",
    "        img = os.path.join(random_train_dir, file)\n",
    "        image = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (96, 96))\n",
    "        train_data.append(image)  # Ajout aux données d'entraînement\n",
    "        train_labels.append(0)    # Label 0 pour non-Oliwer\n",
    "\n",
    "    # Traitement des images de test aléatoires (limité à 60)\n",
    "    random_test_files = [f for f in os.listdir(random_test_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    for file in random_test_files[:600]:\n",
    "        img = os.path.join(random_test_dir, file)\n",
    "        image = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (96, 96))\n",
    "        test_data.append(image)   # Ajout aux données de test\n",
    "        test_labels.append(0)     # Label 0 pour non-Oliwer\n",
    "\n",
    "    return np.array(train_data), np.array(train_labels), np.array(test_data), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b21b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images d'entraînement: 5040\n",
      "Nombre de labels d'entraînement: 5040\n",
      "Nombre d'images de test: 660\n",
      "Nombre de labels de test: 660\n"
     ]
    }
   ],
   "source": [
    "# Préparation des données \n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_data() # Charger les données\n",
    "\n",
    "# Afficher le nombre d'échantillons dans chaque ensemble\n",
    "print(f\"Nombre d'images d'entraînement: {len(train_data)}\")\n",
    "print(f\"Nombre de labels d'entraînement: {len(train_labels)}\") \n",
    "print(f\"Nombre d'images de test: {len(test_data)}\")\n",
    "print(f\"Nombre de labels de test: {len(test_labels)}\")\n",
    "\n",
    "# Vérifier la cohérence des tailles\n",
    "assert len(train_data) == len(train_labels), \"Incohérence entre données et labels d'entraînement\"\n",
    "assert len(test_data) == len(test_labels), \"Incohérence entre données et labels de test\"\n",
    "\n",
    "# Normalisation des données\n",
    "train_data = train_data.astype('float32') / 255.0\n",
    "test_data = test_data.astype('float32') / 255.0\n",
    "\n",
    "# Transformer les images en échelle de gris\n",
    "train_data = np.expand_dims(train_data, axis=-1)\n",
    "test_data = np.expand_dims(test_data, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle\n",
    "# Conv2D = Convolution 2D: Couche de convolution 2D pour extraire les caractéristiques des images\n",
    "# MaxPooling2D = Max Pooling 2D: Réduit la dimension des caractéristiques extraites\n",
    "# Flatten = Aplatir: Transforme les données 2D en un vecteur 1D pour la couche dense\n",
    "# Dense = Couche dense: Couche entièrement connectée pour la classification\n",
    "# Dropout = Abandon: Technique de régularisation pour éviter le surapprentissage\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),   # Dropout plus élevé avant la sortie\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd58e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 45ms/step - accuracy: 0.8948 - loss: 0.2493 - val_accuracy: 0.9576 - val_loss: 0.0966 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.9813 - loss: 0.0480 - val_accuracy: 0.9985 - val_loss: 0.0112 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.9877 - loss: 0.0391 - val_accuracy: 0.9939 - val_loss: 0.0148 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9938 - loss: 0.0162 - val_accuracy: 0.9939 - val_loss: 0.0144 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9952 - loss: 0.0104 - val_accuracy: 0.9985 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9964 - loss: 0.0088 - val_accuracy: 0.9970 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9984 - loss: 0.0051 - val_accuracy: 0.9985 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.9976 - loss: 0.0072 - val_accuracy: 0.9970 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.9980 - loss: 0.0044 - val_accuracy: 0.9985 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.9996 - loss: 0.0019 - val_accuracy: 0.9985 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.9972 - loss: 0.0087 - val_accuracy: 0.9833 - val_loss: 0.0583 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.9982 - loss: 0.0060 - val_accuracy: 0.9939 - val_loss: 0.0096 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.9970 - loss: 0.0090 - val_accuracy: 0.9909 - val_loss: 0.0165 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.9955 - val_loss: 0.0070 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 0.9924 - val_loss: 0.0218 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 3.6028e-04 - val_accuracy: 0.9955 - val_loss: 0.0111 - learning_rate: 2.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.9998 - loss: 5.1690e-04 - val_accuracy: 0.9955 - val_loss: 0.0126 - learning_rate: 2.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 3.6796e-04 - val_accuracy: 0.9985 - val_loss: 0.0027 - learning_rate: 2.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 9.7615e-05 - val_accuracy: 0.9985 - val_loss: 0.0028 - learning_rate: 2.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 9.2178e-05 - val_accuracy: 0.9955 - val_loss: 0.0056 - learning_rate: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle\n",
    "\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    validation_data=(test_data, test_labels),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cd62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9985 - loss: 0.0022\n",
      "Évaluation du modèle: [0.002228757832199335, 0.9984848499298096]\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9985 - loss: 0.0022 \n",
      "Évaluation du modèle chargé: [0.002228757832199335, 0.9984848499298096]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Prédiction: [[1.2473721e-30]]\n",
      "Confiance: 0.00%\n",
      "Ce n'est pas Oliwer avec 100.00% de confiance\n"
     ]
    }
   ],
   "source": [
    "# Évaluation du modèle\n",
    "\n",
    "val = model.evaluate(test_data, test_labels)\n",
    "\n",
    "load_model = keras.models.load_model('model.keras')\n",
    "evaluation = load_model.evaluate(test_data, test_labels)\n",
    "\n",
    "image = cv2.imread('github.png', cv2.IMREAD_GRAYSCALE)\n",
    "image = cv2.resize(image, (100, 100))\n",
    "\n",
    "image = image.astype('float32') / 255.0\n",
    "image = np.expand_dims(image, axis=-1)  \n",
    "image = np.expand_dims(image, axis=0)   \n",
    "\n",
    "prediction = load_model.predict(image)\n",
    "confidence = prediction[0][0]\n",
    "\n",
    "if confidence > 0.5:\n",
    "    print(f\"C'est Oliwer avec {confidence:.2%} de confiance\")\n",
    "else:\n",
    "    print(f\"Ce n'est pas Oliwer avec {(1-confidence):.2%} de confiance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
