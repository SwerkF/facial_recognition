{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ee45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score , confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement des Données\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def generate_batch(image):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    image_normalized = image.astype('float32') / 255.0\n",
    "    image_reshaped = np.expand_dims(image_normalized, axis=-1)\n",
    "    image_batch = np.expand_dims(image_reshaped, axis=0)\n",
    "    \n",
    "    augmented_count = 0\n",
    "    for batch in datagen.flow(image_batch, batch_size=1):\n",
    "        if augmented_count >= 10: \n",
    "            break\n",
    "        \n",
    "        augmented_image = batch[0]\n",
    "        augmented_image = (augmented_image.squeeze() * 255).astype('uint8')\n",
    "        \n",
    "        train_data.append(augmented_image)\n",
    "        train_labels.append(1)\n",
    "        augmented_count += 1\n",
    "\n",
    "    return train_data, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des Données\n",
    "def load_data():\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Données oliwer\n",
    "    for photos in ['oliwer']:\n",
    "        train = f'processed/{photos}/train/' \n",
    "\n",
    "        for file in os.listdir(train):\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                img = os.path.join(train, file) \n",
    "                image = cv2.imread(img, cv2.IMREAD_GRAYSCALE) \n",
    "                image = cv2.resize(image, (96, 96)) \n",
    "                \n",
    "                train_data.append(image)\n",
    "                train_labels.append(1) \n",
    "                \n",
    "                augmented_data, augmented_labels = generate_batch(image)\n",
    "                train_data.extend(augmented_data)\n",
    "                train_labels.extend(augmented_labels)\n",
    "\n",
    "        for file in os.listdir(f'processed/{photos}/test/'):\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                img = os.path.join(f'processed/{photos}/test/', file) \n",
    "                image = cv2.imread(img, cv2.IMREAD_GRAYSCALE) \n",
    "                image = cv2.resize(image, (96, 96))\n",
    "                test_data.append(image)\n",
    "                test_labels.append(1) \n",
    "\n",
    "    # Données Aléatoires\n",
    "    random_train_dir = './processed/non_oliwer/train'  \n",
    "    random_test_dir = './processed/non_oliwer/test'   \n",
    "    \n",
    "    # Traitement des images d'entraînement aléatoires (limité à 240)\n",
    "    random_train_files = [f for f in os.listdir(random_train_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    for file in random_train_files[:2400]:\n",
    "        img = os.path.join(random_train_dir, file)\n",
    "        image = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (96, 96))\n",
    "        train_data.append(image)  \n",
    "        train_labels.append(0)   \n",
    "\n",
    "    # Traitement des images de test aléatoires (limité à 60)\n",
    "    random_test_files = [f for f in os.listdir(random_test_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    for file in random_test_files[:600]:\n",
    "        img = os.path.join(random_test_dir, file)\n",
    "        image = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (96, 96))\n",
    "        test_data.append(image)  \n",
    "        test_labels.append(0)  \n",
    "\n",
    "    return np.array(train_data), np.array(train_labels), np.array(test_data), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des Données \n",
    "train_data, train_labels, test_data, test_labels = load_data() \n",
    "\n",
    "train_data = train_data.astype('float32') / 255.0\n",
    "test_data = test_data.astype('float32') / 255.0\n",
    "\n",
    "train_data = np.expand_dims(train_data, axis=-1)\n",
    "test_data = np.expand_dims(test_data, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d5590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback\n",
    "callbacks_fixed = [\n",
    "    TensorBoard(log_dir='logs', histogram_freq=1),\n",
    "    ModelCheckpoint('model2.keras', save_best_only=True),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Conception du Model \n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3,3), padding='same', activation='relu', input_shape=(96,96,1)),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "history = model.fit(train_data, train_labels, epochs=50, validation_data=(test_data, test_labels), batch_size=32, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dce417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du Modèle\n",
    "predictions = model.predict(test_data)\n",
    "predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "print(classification_report(test_labels, predicted_labels, target_names=label_encoder.classes_))\n",
    "print(\"F1 Score:\", f1_score(test_labels, predicted_labels, average='weighted'))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(test_labels, predicted_labels))\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "import seaborn as sns\n",
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "val = model.evaluate(test_data, test_labels)\n",
    "\n",
    "load_model = keras.models.load_model('model.keras')\n",
    "evaluation = load_model.evaluate(test_data, test_labels)\n",
    "\n",
    "image = cv2.imread('test3.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "image = cv2.resize(image, (96, 96))\n",
    "\n",
    "image = image.astype('float32') / 255.0\n",
    "image = np.expand_dims(image, axis=-1)  \n",
    "image = np.expand_dims(image, axis=0)   \n",
    "\n",
    "prediction = load_model.predict(image)\n",
    "confidence = prediction[0][0]\n",
    "\n",
    "if confidence > 0.5:\n",
    "    print(f\"C'est Oliwer avec {confidence:.2%} de confiance\")\n",
    "else:\n",
    "    print(f\"Ce n'est pas Oliwer avec {(1-confidence):.2%} de confiance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
